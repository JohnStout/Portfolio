---
title: "Assignment #5 (Final)"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
```

# John Stout

PLEASE READ INSTRUCTIONS CAREFULLY. There are three major parts to this assignment. Part I deals with power analysis and other basic aspects of analysis. Part II involves a repeated-measures dataset that I have cast as a mixed design, and assesses your ability to organize and filter data, and make plots. Part III is a relatively straightforward mixed-design analysis.

Instructions: Make a folder on your computer and save this file in it along with the data sets. Change your working directory (manually, please don't include code for doing that in your RMD file) to that directory. Place your answers underneath the corresponding question, flanked by ** in order to make them show up as bold text, except for code segments.

For power analyses, I want you to take screenshots of GPower (unless you decide to use R, in which case just show your code). You can save those screenshots of the GPower window as e.g. 1b.jpg, and then insert them into your R markdown file with a command like the below:

This will knit the image into the resulting document. To take screenshots, you can use windows' snippet tool or Mac's built-in tools (see links below if you don't know how to do this).
Windows:
Mac: https://support.apple.com/en-us/HT201361
Windows: https://www.take-a-screenshot.org/windows.html

ONCE YOU ARE FINISHED (make sure document knits), zip up the whole folder (RMD, images, HTML, etc) and upload that folder to Canvas.

# I. Power and designs

You can use GPower (include screenshots) or the pwr package in R to help you answer any questions regarding power, except where noted. I recommend GPower simply because I relied on it, for the most part, in class. You may also use a simulation if you wish, but none of these questions require that.

1. Suppose we are interested in visual search performance (reaction time, RT) for food targets under two conditions: fasted and non-fasted. The hypothesis is that fasting will improve search for food targets. We have the same individuals perform search on different days (counterbalanced order), and average performance to come up with one RT score related to fasted performance, and another related to non-fasted performance, for each individual. 

a. What is the logical statistical test to perform here? 

**Given that we are generating a dataset where each subject has both a trial-averaged fasted RT and a trial-averaged non-fasted RT, we would perform either a paired t-test or wilcoxon signed rank test depending on the normality of the data. ** 

b. Based on the literature, we estimate an effect size of d = .45. How many subjects will be required to have an approximately 80% chance of detecting an effect of that magnitude? Show your work in a screenshot or an R code segment.

**This study should require 41 subjects for an effect size of d = 0.45 and an 80% change of detecting the effect.**
![](1b.jpg)

2. Suppose that we run a follow-up to the previous study. We decide to include three levels of fasting, and test these levels on entirely different sets of subjects. We get one score from each participant. 

a. What is the logical (omnibus) statistical test to perform here?

**The logical (omnibus) test to perform here would be a one-way ANOVA, then if we observe a main effect or interaction, to follow up the ANOVA with unpaired, pairwise testing to determine where the effects are observed. ** 

b. Suppose that we estimate that the effect of fasting levels will be approximately eta-squared of .22. How many subjects will be required to have an approximately 90% chance of detecting an effect of that magnitude? Show your work in a GPower picture and/or R code segment. 

**This study would require a total of 267 subjects to meet the criteria of 0.90 power and an et-squared of 0.22 **
![](2b.jpg)


3. Suppose that we run a follow-up to the previous study. Now we are going to separate fasting vs. non-fasting participants into separate groups. But we are also going to test every subject on both search for food targets and search for non-food targets. We will average performance for each of the conditions to produce two scores for each person.

a. What is the logical (omnibus) statistical test to perform here? What are your factors and what kind of factor is each (repeated or between-groups)?

**The logical (omnibus) test would be a mixed ANOVA with one between group factor and one within group factor. The between groups factor is fasted and non-fasted (two groups), and the within groups factor (or repeated measure) is the reaction time for food or non-food targets. **

b. Suppose you want to power your study such that you will have a 95% chance to detect an interaction effect between fasting state and target type (effect size partial eta squared = .14). Assume that the correlation amongst any repeated measures factor is .75. How many subjects do you need (total)? Show work with either GPower screenshot or R code.

**According to my power analysis, this study would require 14 subjects in total (7 per group)**
![](3b.jpg)

c. Suppose that the design was modified to include 3 different target types (food, food-related, and non-food-related) for each participant. Otherwise the study is the same. Now you wish to power your study such that you will have an 80% power to detect a main effect of target type (effect size partial eta squared = .14). How many subjects do you need (total)? Show work with either GPower screenshot or R code.

**We would require 8 subjects in total (4 per group).**

![](3c.jpg)

4. Suppose that we run another follow-up to the previous studies. Now we are going to separate fasting vs. non-fasting participants into separate groups, and we are also going to test every subject on both search for food targets and search for non-food targets. We are also adding an additional twist: distractors will be either food or non-food stimuli, crossed twith target type. We will average performance for each of the combinations of conditions to produce four scores per person. What type of ANOVA does this design suggest we should employ? There is no power analysis required for this (would necessitate simulations approach).

**We should employ an ANCOVA, treating the distractor stimuli as the covariate. **

# II. Dataset 1

This data comes from my lab, and was a pilot study for a project involving "object-based attention." In this experiment, we are interested in an effect called "object-based warping,"" and how this effect might relate to "object-based attention." I don't think it will be necessary for you to understand too much about this paradigm (I go into some rudimentary detail below), but if you find yourself wanting some additional background, here is a link to an object-based attention review (you may need to be on-campus, VPN'd, or proxify the link to view):

https://link.springer.com/article/10.3758/s13414-012-0322-z

Visual selective attention is known to be "space-based" in the sense that, if I flash a light in a given part of a visual scene, your attention will temporarily concentrate on that region and you will have greater awareness of and enhanced processing of stimuli at that location (the famous "Posner Cueing Task"). You can also willfully distribute attention to parts of space (e.g., stare straight ahead but attend to the left side of your visual world -- processing of things on the left is enhanced vs. the right side of visual space when you do this). Attention is also "feature-based" in that you can select for specific features in the entire scene. E.g., if I'm searching for a coca-cola bottle, I can attend to things in the visual scene that are red, consistent with the coca-cola logo's color. Finally, attention has also been proposed to be "object-based," in part, as well. Meaning that, if I attend to one part of an object, I will tend to select the entire object with my attention. E.g., attending to someone's arm entails attending, to some degree, to their entire body.

Object-based attention has been demonstrated in several ways, but the way used here is based on Egly, Driver, and Rafal (1994), a well-cited article that used a cueing procedure. What they did was present the subject on each trial with two "objects" (just two vertical or horizontal rectangles, really). The task was to respond quickly and accurately to a probe that could appear on one of the ends of the two rectangles. Shortly before the probe appeared, a "cue" briefly highlighted one of the ends of one of the rectangles. The key manipulation was the location of the cue relative to the probe. "Valid" trials (the majority) placed the cue on the same location as the probe. "Invalid-same" trials placed the cue on the same object, but the opposite side as the probe. "Invalid-different" trials placed the cue on the other object vs. the probe, but in such a way that all "Invalid-different" trials had the same cue-probe distance as "invalid-same" trials. The key finding is that responses to "invalid-same" trials is faster and more accurate, overall, than "invalid-different." This implies preferential treatment of the equidistant same-object location over the different-object location. I.e., the cue causes selection of the entire object. Note, however, that valid trials are the fastest, implying a non-object-based, spatial focus of attention. Here is a picture from an online presentation that depicts this: 

https://slideplayer.com/slide/7538111/24/images/40/Results+Egly%2C+Driver%2C+%26+Rafal+%281994%29.jpg

Finally, our study: participants completed an object-based attention task in one part of the experiment. In our study, participants completed two tasks, an "object-based warping" task and a "object-based attention" task, in a counterbalanced order. In the "object-based attention" blocks, the observers saw two rectangles (either vertically or horizontally oriented). One end of one rectangle was cued, and then stimuli appeared in all corners. The target was either a T or L, and the distractors were hybrids of T and L shapes. The target always appeared at either the same location as the cue ("valid"), or a different location that was either on the cued object ("same") or at an equidistant location on the other object ("diff"). I'm not going to go into detail about the other task, but let's presume that we are concerned that potentially the order in which subjects did the tasks might have had an impact on the "object-based attention" effect (there was actually no concern on my part, but we will find out if there should have been).

Throughout the following be aware of the need to verify that factors are coded as factors. I will not tell you every single step you need to complete to accomplish your goal -- e.g., you may need to compute new summary data frames in order to plot something, but I won't walk you through that here.

5. The data are in the oba.csv file that was contained in the zip file. Use code segments below each lettered part of this question to respond.

a. Load the data into a variable called oba_df. Inspect oba_df by using the head function or looking at it in your environment tab. Again, this is raw data in long format. I have included a variable coding for block and one coding for trial within block. The orientation column describes whether the two rectangles were oriented horizontally or vertical on this trial (this factor was sort of blocked into mini-runs, which is why you see runs of the same). cueType was valid (same location), same (different location, same object), or diff (different loc, different object). There is both an accuracy and a rt column for reaction time. We are just going to concern ourselves with same and diff trials, so go ahead and filter out all valid trials (if you look at the means for valid, you'll notice they are substantially faster than the other two conditions, as expected).

```{r}
library(tidyverse)

# define - inspected in tab environment
oba_df = read.csv("oba.csv") 

# save an og var
oba_df_og = oba_df

# remove valid
oba_df_new = oba_df %>% filter(cueType != "valid")

```

b. Calculate overall accuracy for each of the subjects. This task is easy, so performance should be very high. Get rid (entirely) of all subjects who are, on average, less than 80% correct (i.e., eliminate those subjects from oba_df). 

```{r}
# average within subjects (over-all accuracy)
accuracy_mean = oba_df_new %>% 
  group_by(subID) %>%
  summarise_at(c("accuracy"),funs(mean,sem=sd(.)/sqrt(n())))

# make an index to remove data
idx_rem = which(accuracy_mean$mean < 0.8)

# remove the subjects
oba_df_rem = oba_df_new
for (i in idx_rem)
{
oba_df_rem = oba_df_rem %>% filter(subID != i)
}

# save
oba_df = oba_df_rem

```

c. Now, we need to do further cleaning of oba_df. First, eliminate all error trials -- we will focus on RT. Next, for each combination of orientation and cueType, trim outliers that are 3.0 standard deviations above or below the mean for each subject *within that cell* of the design. I'm not asking you to remove outlying *subjects*, mind you. Remove outlying values *for each subject* and with respect to their own responses. Each subject should have only some values removed, if any, so all subjects should still have a value for each cell of the design. 

```{r}
# store second time
oba_df_og2 = oba_df

# 1. remove all error trials
oba_df = oba_df %>% filter(accuracy != 0)

# 2. trim outliers that are z = 3 or z = -3 for each subject for each combination of orientation and cueType - use scale to zscore. Remove outliers responses that are 3 std away from mean.
oba_df$zscore_rt = oba_df$rt # prep the variable for mutate
oba_df = oba_df %>% 
  group_by(subID,orientation,cueType) %>% 
  mutate(zscore_rt = scale(rt))
oba_df = oba_df %>% filter(zscore_rt < 3 & zscore_rt > -3) # remove

# checks
check1 = which(oba_df$zscore_rt > 3)
check2 = which(oba_df$zscore_rt < -3)

print(check1); print(check2);

```

d. Notice that some people have different block numbers than others. That's because some people completing object-based attention tasks in blocks 1 and 4, while others completed it in blocks 2 and 3. We are going to treat this as a between-subjects factor. We need to create a code that tells us the condition the subject was in. In oba_df, create orderCond, a column that codes as "first" if the blocks were 1 and 4, and "second" if the blocks were 2 and 3.

```{r}
# confirm the announcement
block_names = unique(oba_df$block)

# make indices - 'code 0/3 as first and 1/2 as second'
idx_second = which(oba_df$block == 1 | oba_df$block == 2)
idx_first  = which(oba_df$block == 0 | oba_df$block == 3)

# temp variable
tot_len = length(idx_first)+length(idx_second)
tempvar = seq(from = 1, to = tot_len, by = 1)

# replace
tempvar[idx_first] = "first"
tempvar[idx_second] = "second"

# make data array addition
oba_df$orderCond = tempvar

```

d. Inspect boxplots of RT means collapsing across cueType and orientation (i.e, just one box). Does anyone look like an outlier? Don't do anything about it, though, if there is an outlying person.

```{r}
# figure variable

# this variable is collapsed across cueType and orientation, but not orderCond
oba_graph = oba_df %>% 
  group_by(subID,orderCond) %>%
  summarise_at(c("rt"),funs(mean,sem=sd(.)/sqrt(n())))

ggplot(oba_graph, aes(y = mean, x = orderCond, fill = orderCond)) + 
  geom_boxplot()+theme_classic()

```
**It seems that one subject may have been an outlier in the first orderCond group. **

e. Make a bar plot for mean RT in each condition, where there are separate bars for each cueType and orientation combination. The bars should also be separated according to whether the subject belonged to the "first" or "second" group. Make sure to include standard errors. Does the anticipated effect (same RT < diff RT) appear evident  in this plot, and does it appear to be present for both groups and in both orientations?

```{r}
# First average across all trials
trial_avg = oba_df %>% 
  group_by(subID,orientation,cueType,orderCond) %>%
  summarise_at(c("rt"),funs(avg=mean,sem=sd(.)/sqrt(n())))
trial_avg$cue_orien = paste(trial_avg$cueType, trial_avg$orientation)

# Next average across individuals for each condition
sub_avg = trial_avg %>%
  group_by(orientation,cueType,orderCond)%>%
  summarise_at(c("avg"),funs(mean,sem=sd(.)/sqrt(n())))
sub_avg$cue_orien = paste(sub_avg$cueType, sub_avg$orientation)
  
  
ggplot(sub_avg, aes(y = mean, x = cue_orien))+
  geom_bar(stat="identity", color="black", position=position_dodge()) +
  geom_errorbar(aes(ymin = mean-sem, ymax = mean+sem), width=.2, position=position_dodge(.9)) +
  theme_classic() + facet_grid(. ~ orderCond)

```

**Based on the observed data, it seems that individuals who experienced the object based attention task second may have faster RTs regardless of whether it was same vertical or not. This suggests that we may observe a main effect of orderCond. With respect to the same RT being less than diff RT, those who experienced the attention task first have noticibly faster same vertical response times with respect to other orientations. We may also see that same RT is less than diff RT in those who experienced the attention task second, but this may be because diff horizontal took them longer than diff vertical, thus pulling the mean upwards (or diff vertical pulled the mean downwards).**

f. Run an ANOVA with orderCond, cueType, and orientation as factors. Run follow-up tests as appropriate. Write up in APA format.

```{r}
library(ez)
trial_avg$orderCond   = as.factor(trial_avg$orderCond)
trial_avg$cueType     = as.factor(trial_avg$cueType)
trial_avg$orientation = as.factor(trial_avg$orientation)
trial_avg$cue_orien   = as.factor(trial_avg$cue_orien)

data_anova = ezANOVA(
  data               = trial_avg,
  dv                 = avg,
  wid                = subID,
  within             = .(orientation,cueType),
  between            = orderCond,
  detailed           = TRUE,   
  type               = 3
  )

print(data_anova)


# isolate first and second
df_fir = trial_avg %>% filter(orderCond != "first")
df_fir = df_fir %>% 
  group_by(subID,orientation,cueType) %>%
  summarise_at(c("avg"),funs(mean,sem=sd(.)/sqrt(n())))
df_fir$cue_orien = paste(df_fir$cueType, df_fir$orientation)


df_sec = trial_avg %>% filter(orderCond != "second")
df_sec = df_sec %>% 
  group_by(subID,orientation,cueType) %>%
  summarise_at(c("avg"),funs(mean,sem=sd(.)/sqrt(n())))
df_sec$cue_orien = paste(df_sec$cueType, df_sec$orientation)

# ttests - I feel like I've done something wrong, but I can't quite figure out where my error is. A p value of 1.0 is strange, and the wording makes me think an effect should be somewhere. Also based on the (little) knowledge that I have of this task, I should expect to observe the same condition to be less than the rest.
p_fir = pairwise.t.test(df_fir$mean, df_fir$cue_orien, paired=TRUE, p.adjust.method="holm")

p_sec = pairwise.t.test(df_sec$mean, df_sec$cue_orien, paired=TRUE, p.adjust.method="holm")

print(p_fir)
print(p_sec)
```

**To begin to understand if the order by which subjects experienced the attention task affected their performance on the object based attention performance, we ran a repeated measures anova with orderCond as the between subject factor. We found a main effect of orientation (F(1,32) = 6.34, p = 0.017) and cueType (F(1,32) = 9.08, p = 0.005). This suggests that depending on the cue that they received, and the orientation by which the stimuli appeared, individuals performed differently. To test where the major differences lied, we then ran pairwise t-tests with holm corrected p-values. However, we found no significant changes between any combination of comparisons (see printed tables above).**

g. Based on f, is the effect of object-based attention consistent in both groups? Is it dependent on orientation of the bars? Just descriptive summary here.

**Based on my analyses that I mentioned in f, the lack of effect that I observed is consistent across both groups. Given that I found no significant differences between any groups, the orientation of the bars did not influence the performance.**


# III. Dataset 2

One last dataset problem. This one is a bit simpler than the above. Here is a quote from the source I drew this data from: "This fake data is a sample of 15 younger adults and 15 older adults. Similar to our previous example, all subjects were initially shown a set of words. Afterwards, for some of the words they were given a multiple choice test, and the rest of the words they simply restudied. After a 2-day delay, their memory was tested for all words to determine if there were memory differences depending on whether the word was tested or restudied. The dependent variable (DV) given in the file are a proportion of correctly remembered items for the delayed memory test for both conditions. "
 
6. The data are in dataset2.csv. 

a. Load the data into part3_df. Recode the Btw_Cond and Within_Cond fields to better reflect the underlying conditions (e.g., "young" instead of 0, as a factor). Young people are represented by 0 in Btw_Cond vs. 1 for Older adults. Test condition is 1 while restudy is 2 in the Within_Cond. 

```{r}
# load
part3_df = read.csv("dataset2.csv")

# recode Btw_Cond and Within_Cond fields to better reflect the underlying conditions ("young" in lieu of 0, as a factor)

part3_df$Btw_Cond[which(part3_df$Btw_Cond == 0)] = 'young'
part3_df$Btw_Cond[which(part3_df$Btw_Cond == 1)] = 'old'
part3_df$Within_Cond[which(part3_df$Within_Cond == 1)] = 'test'
part3_df$Within_Cond[which(part3_df$Within_Cond == 2)] = 'restudy'

# convert to factor
part3_df$Btw_Cond = as.factor(part3_df$Btw_Cond)
part3_df$Within_Cond = as.factor(part3_df$Within_Cond)

```

b. Make box plots of DV for each condition combination. Any concerns raised by these plots?

```{r}
# plot
ggplot(part3_df, aes(y = DV, x = Btw_Cond, fill = Within_Cond)) + 
  geom_boxplot()+theme_classic()
```

**Based on the box plot for the young group and tested words, the data could be not normally distributed. This is suspected because 1) the two tails of the box plots are not even in length, 2) the presence of an outlier may suggest data non-normality.**

c. Make a summary dataframe and use it to create a barplot with standard-error error bars. 

```{r}
# dataframe
sum_df = part3_df %>% 
  group_by(Btw_Cond,Within_Cond) %>%
  summarise_at(c("DV"),funs(avg=mean,sem=sd(.)/sqrt(n())))

# barplot
ggplot(sum_df, aes(y = avg, x = Btw_Cond, fill = Within_Cond))+
  geom_bar(stat="identity", color="black", position=position_dodge()) +
  geom_errorbar(aes(ymin = avg-sem, ymax = avg+sem), width=.2, position=position_dodge(.9)) +
  theme_classic()

```

d. Show how to make a "wide" version of this data in a variable called part3_wide. You don't have to save it, though you could do so on your own and use it in JASP or Jamovi to check your work.

```{r}
# wide format
data_wide = spread(part3_df, Within_Cond, DV)

```

e. Do a full analysis in R using ANOVA and appropriate follow-up tests.

```{r}
# anova
part3_anova = ezANOVA(
  data               = part3_df,
  dv                 = DV,
  wid                = Subject,
  within             = Within_Cond,
  between            = Btw_Cond,
  detailed           = TRUE,   
  type               = 3
  )
print(part3_anova)

# I could not figure out how to do mixed design pairwise t-tests, so i did so manually. This way I could correct for p-values
young_test = part3_df$DV[which(part3_df$Btw_Cond == "young" & part3_df$Within_Cond == "test")]
young_ret = part3_df$DV[which(part3_df$Btw_Cond == "young" & part3_df$Within_Cond == "restudy")]
t_paired_young = t.test(young_test,young_ret,paired=TRUE)

old_test = part3_df$DV[which(part3_df$Btw_Cond == "old" & part3_df$Within_Cond == "test")]
old_ret = part3_df$DV[which(part3_df$Btw_Cond == "old" & part3_df$Within_Cond == "restudy")]
t_paired_old = t.test(old_test,old_ret,paired=TRUE)

t_both_test = t.test(young_test,old_test,paired=FALSE)
t_both_ret  = t.test(young_ret,old_ret,paired=FALSE)

# make vector of pvalues
p_vec = c(t_paired_young$p.value,t_paired_old$p.value,t_both_test$p.value,t_both_ret$p.value)
p_corrected = round(p.adjust(p_vec,"bonferroni"),3)

# make data frame
p_df = data.frame("DV_young" = p_corrected[1],
                  "DV_old" = p_corrected[2],
                  "test_both" = p_corrected[3],
                  "retest_both" = p_corrected[4])
# t-stat dataframe
t_df =  data.frame("DV_young" = t_paired_young[["statistic"]][["t"]],
                  "DV_old" = t_paired_old[["statistic"]][["t"]],
                  "test_both" = t_both_test[["statistic"]][["t"]],
                  "retest_both" = t_both_ret[["statistic"]][["t"]])

deg_free = data.frame("DV_young" = t_paired_young[["parameter"]][["df"]],
                  "DV_old" = t_paired_old[["parameter"]][["df"]],
                  "test_both" = t_both_test[["parameter"]][["df"]],
                  "retest_both" = t_both_ret[["parameter"]][["df"]])

```

f. Write up your results in APA format.

**In this study, we were interested in examining whether memory of words was stronger when recalling them after experiencing them on a test, or after an epoch of study. We also wondered if age impacting the ability for these differing experiences to impact memory recall. Therefore, we first ran a mixed anova with age as a between subject variable and recall performance on words that were either tested or studied 2 days prior. We first found a main effect of age (F(1,28) = `r round(part3_anova[["ANOVA"]][["F"]][[2]],2)`, p = `r round(part3_anova[["ANOVA"]][["p"]][[2]],3)`, generalized eta squared = `r round(part3_anova[["ANOVA"]][["ges"]][[2]],3)`) and of the subjects experience 2-days prior to recall (F(1,28) = `r round(part3_anova[["ANOVA"]][["F"]][[3]],2)`, p = `r round(part3_anova[["ANOVA"]][["p"]][[3]],3)`, generalized eta squared = `r round(part3_anova[["ANOVA"]][["ges"]][[3]],3)`). Further, there was an interaction between age and word-experience (F(1,28) = `r round(part3_anova[["ANOVA"]][["F"]][[4]],2)`, p = `r round(part3_anova[["ANOVA"]][["p"]][[4]],3)`, generalized eta squared = `r round(part3_anova[["ANOVA"]][["ges"]][[4]],3)`), likely due to young subjects seeming to perform better on the testing condition 2 days prior (see Part3C figure). We then followed up with paired t-tests within each group, and Welches t-tests between groups using bonferroni corrected p-values. Note that young individuals recall words better when tested on them, than if left to study (t(`r round(deg_free[[1]],2)`) = `r round(t_df[[1]],2)`, p = `r round(p_df[[1]],3)`, paired t-test; mean test = `r round(mean(young_test),2)`, std = `r round(sd(young_test),2)`; mean restudy = `r round(mean(young_ret),2)`, std = `r round(sd(young_ret),2)`). Additionally, young individuals performed significantly better than old individuals when recalling words that they were previously tested on (t(`r round(deg_free[[3]],2)`) = `r round(t_df[[3]],2)`, p = `r round(p_df[[3]],3)`, Welches unpaired t-test; mean young test = `r round(mean(young_test),2)`, std = `r round(sd(young_test),2)`; mean old test = `r round(mean(old_test),2)`, std = `r round(sd(old_test),2)`). This data suggests that young individuals are better at recalling words if they experience them in a testing format, with respect to a study format.**
